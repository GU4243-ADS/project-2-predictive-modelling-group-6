train_total <- merge(train_total,train_LBP,by=c("Image"))
trainx1 <- train_total[1:nrow(train_total),2:ncol(train_total)]
trainx2 <- data.matrix(trainx1, rownames.force = NA)
trainx2 <- unname(trainx2, force = FALSE)
test_total <- merge(test_color,test_HOG,by=c("Image"))
test_total <- merge(test_total,test_LBP,by=c("Image"))
testx1 <- test_total[1:nrow(test_total),2:ncol(test_total)]
testx2 <- data.matrix(testx1, rownames.force = NA)
testx2 <- unname(testx2, force = FALSE)
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 512, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.4) %>%
#  layer_dense(units= 512, activation = "relu") %>%
layer_dense(units= 256, activation = "relu") %>%
#  layer_dropout(rate = 0.3) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 50,
batch_size = 32,
validation_data = (testx2,testlabels)) #20% data used for training
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 50,
batch_size = 32,
validation_data = (testx2, testlabels)) #20% data used for training
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 50,
batch_size = 32,
validation_data = list(testx2, testlabels))
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
# If you are using anaconda, point reticulate to the correct conda environment
# use_condaenv('your-environment')
# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
#cv2 <- reticulate::import('cv2')
library(EBImage)
library(tensorflow)
library(keras)
# Read the seperated data
load("../data/split_data/train/train_HOG.RData")
load("../data/split_data/train/train_label.RData")
load("../data/split_data/train/train_color.RData")
load("../data/split_data/train/train_LBP.RData")
load("../data/split_data/test/test_HOG.RData")
load("../data/split_data/test/test_label.RData")
load("../data/split_data/test/test_color.RData")
load("../data/split_data/test/test_LBP.RData")
trainy1 <- list()
for (i in 1:nrow(train_label)) {
if (train_label$label[i]=="dog"){
trainy1[i] <-1
}
else{
trainy1[i] <- 0
}
}
testy1 <- list()
for (i in 1:nrow(test_label)) {
if (test_label$label[i]=="dog"){
testy1[i] <-1
}
else{
testy1[i] <- 0  }
}
#Create data is a 3-d array of grayscale values. To prepare data fro training, it has to convert the 3-d arrays into matrices by reshapinf width dand heights into a single dimension. Then convert the grayscale values from integers ranging between 0 to 255 into floating point value between 0 to 1.
train_total <- merge(train_color,train_HOG,by=c("Image"))
train_total <- merge(train_total,train_LBP,by=c("Image"))
trainx1 <- train_total[1:nrow(train_total),2:ncol(train_total)]
trainx2 <- data.matrix(trainx1, rownames.force = NA)
trainx2 <- unname(trainx2, force = FALSE)
test_total <- merge(test_color,test_HOG,by=c("Image"))
test_total <- merge(test_total,test_LBP,by=c("Image"))
testx1 <- test_total[1:nrow(test_total),2:ncol(test_total)]
testx2 <- data.matrix(testx1, rownames.force = NA)
testx2 <- unname(testx2, force = FALSE)
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 512, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.4) %>%
#  layer_dense(units= 512, activation = "relu") %>%
layer_dense(units= 256, activation = "relu") %>%
#  layer_dropout(rate = 0.3) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 50,
batch_size = 32,
validation_data = list(testx2, testlabels))
remove(mode)
remove(model)
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 1024, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.3) %>%
#  layer_dense(units= 512, activation = "relu") %>%
layer_dense(units= 256, activation = "relu") %>%
#  layer_dropout(rate = 0.3) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 50,
batch_size = 32,
validation_data = list(testx2, testlabels))
# model testing
model %>% evaluate(testx2,testlabels)
pred <- model%>%predict_classes(testx2)
clear
clear(model)
# Read the seperated data
load("../data/split_data/train/train_HOG.RData")
load("../data/split_data/train/train_label.RData")
load("../data/split_data/train/train_color.RData")
load("../data/split_data/train/train_LBP.RData")
load("../data/split_data/test/test_HOG.RData")
load("../data/split_data/test/test_label.RData")
load("../data/split_data/test/test_color.RData")
load("../data/split_data/test/test_LBP.RData")
trainy1 <- list()
for (i in 1:nrow(train_label)) {
if (train_label$label[i]=="dog"){
trainy1[i] <-1
}
else{
trainy1[i] <- 0
}
}
testy1 <- list()
for (i in 1:nrow(test_label)) {
if (test_label$label[i]=="dog"){
testy1[i] <-1
}
else{
testy1[i] <- 0  }
}
#Create data is a 3-d array of grayscale values. To prepare data fro training, it has to convert the 3-d arrays into matrices by reshapinf width dand heights into a single dimension. Then convert the grayscale values from integers ranging between 0 to 255 into floating point value between 0 to 1.
train_total <- merge(train_color,train_HOG,by=c("Image"))
train_total <- merge(train_total,train_LBP,by=c("Image"))
trainx1 <- train_total[1:nrow(train_total),2:ncol(train_total)]
trainx2 <- data.matrix(trainx1, rownames.force = NA)
trainx2 <- unname(trainx2, force = FALSE)
test_total <- merge(test_color,test_HOG,by=c("Image"))
test_total <- merge(test_total,test_LBP,by=c("Image"))
testx1 <- test_total[1:nrow(test_total),2:ncol(test_total)]
testx2 <- data.matrix(testx1, rownames.force = NA)
testx2 <- unname(testx2, force = FALSE)
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 2000, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.3) %>%
#  layer_dense(units= 512, activation = "relu") %>%
layer_dense(units= 1000, activation = "relu") %>%
#  layer_dropout(rate = 0.3) %>%
layer_dense(units= 200, activation = "relu") %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 50,
batch_size = 32,
validation_data = list(testx2, testlabels))
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 20,
batch_size = 32,
validation_data = list(testx2, testlabels))
# model testing
model %>% evaluate(testx2,testlabels)
pred <- model%>%predict_classes(testx2)
clear(model)
clear(value)
clear("model")
# Read the seperated data
load("../data/split_data/train/train_HOG.RData")
load("../data/split_data/train/train_label.RData")
load("../data/split_data/train/train_color.RData")
load("../data/split_data/train/train_LBP.RData")
load("../data/split_data/test/test_HOG.RData")
load("../data/split_data/test/test_label.RData")
load("../data/split_data/test/test_color.RData")
load("../data/split_data/test/test_LBP.RData")
trainy1 <- list()
for (i in 1:nrow(train_label)) {
if (train_label$label[i]=="dog"){
trainy1[i] <-1
}
else{
trainy1[i] <- 0
}
}
testy1 <- list()
for (i in 1:nrow(test_label)) {
if (test_label$label[i]=="dog"){
testy1[i] <-1
}
else{
testy1[i] <- 0  }
}
#Create data is a 3-d array of grayscale values. To prepare data fro training, it has to convert the 3-d arrays into matrices by reshapinf width dand heights into a single dimension. Then convert the grayscale values from integers ranging between 0 to 255 into floating point value between 0 to 1.
train_total <- merge(train_color,train_HOG,by=c("Image"))
train_total <- merge(train_total,train_LBP,by=c("Image"))
trainx1 <- train_total[1:nrow(train_total),2:ncol(train_total)]
trainx2 <- data.matrix(trainx1, rownames.force = NA)
trainx2 <- unname(trainx2, force = FALSE)
test_total <- merge(test_color,test_HOG,by=c("Image"))
test_total <- merge(test_total,test_LBP,by=c("Image"))
testx1 <- test_total[1:nrow(test_total),2:ncol(test_total)]
testx2 <- data.matrix(testx1, rownames.force = NA)
testx2 <- unname(testx2, force = FALSE)
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 2000, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.3) %>%
#  layer_dense(units= 512, activation = "relu",use_bias = T) %>%
layer_dense(units= 1000, activation = "relu",use_bias = T) %>%
#  layer_dropout(rate = 0.3) %>%
layer_dense(units= 200, activation = "relu",use_bias = T) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 20,
batch_size = 32,
validation_data = list(testx2, testlabels))
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 1024, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.3) %>%
#  layer_dense(units= 512, activation = "relu",use_bias = T) %>%
layer_dense(units= 512, activation = "relu",use_bias = T) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units= 64, activation = "relu",use_bias = T) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 20,
batch_size = 32,
validation_data = list(testx2, testlabels))
# Training visualization
plot(history)
# as.data.frame(history)
# model testing
model %>% evaluate(testx2,testlabels)
pred <- model%>%predict_classes(testx2)
n
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 1024, activation = "relu", input_shape = c(n)) %>%
layer_dropout(rate = 0.4) %>%
#  layer_dense(units= 512, activation = "relu",use_bias = T) %>%
layer_dense(units= 128, activation = "relu",use_bias = T) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units= 32, activation = "relu",use_bias = T) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 20,
batch_size = 32,
validation_data = list(testx2, testlabels))
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)
#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers.
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352
model <- keras_model_sequential()
model %>%
layer_dense(units= 512, activation = "relu", input_shape = c(n)) %>%
#  layer_dropout(rate = 0.4) %>%
#  layer_dense(units= 512, activation = "relu",use_bias = T) %>%
layer_dense(units= 128, activation = "relu",use_bias = T) %>%
#  layer_dropout(rate = 0.3) %>%
layer_dense(units= 32, activation = "relu",use_bias = T) %>%
layer_dense(units= 2, activation = "softmax")
# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid
summary(model)
# Compile
# The model with approriate loss function, optimizer and metrics
model %>%
compile(loss ='binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c("accuracy"))
#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually
# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression
# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.
history <- model %>%
fit(trainx2,
trainlabels,
epochs = 20,
batch_size = 32,
validation_data = list(testx2, testlabels))
# Training visualization
plot(history)
# as.data.frame(history)
