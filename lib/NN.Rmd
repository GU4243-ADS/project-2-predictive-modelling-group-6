---
title: "Classification method"
author: "Kai"
date: "February 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(reticulate)

# If you are using anaconda, point reticulate to the correct conda environment
# use_condaenv('your-environment')

# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
#cv2 <- reticulate::import('cv2')

library(EBImage)
library(tensorflow)
library(keras)
library(caret)
```

```{r}
# Read the seperated data
load("../data/split_data/train/train_HOG.RData")
load("../data/split_data/train/train_label.RData")
load("../data/split_data/train/train_color.RData")
load("../data/split_data/train/train_LBP.RData")

load("../data/split_data/test/test_HOG.RData")
load("../data/split_data/test/test_label.RData")
load("../data/split_data/test/test_color.RData")
load("../data/split_data/test/test_LBP.RData")
```
##Read images


```{r}
# set the working direction
#setwd("../data/pets_training/pets/train")
# load pictures one by one
#pics <- c("sss")


ID <- list(1:2000)
label <- read.table("../output/train_label_11.txt")

imgdata <- data.frame(ID,label)
s1 <- "pet"
s3 <- ".jpg"

pics <- list()
for(i in 1:2000){
  s2 <- toString(i)
  pics[[i]] <- paste(s1, s2, s3,sep = "")
}


mypic <- list()
for (i in 1:length(pics)) {
  mypic[[i]] <- readImage(paste("../data/pets_training/pets/train/",pics[i],sep=""))
  }


```

# Data Processing
```{r}
# Resize
for (i in 1:length(pics)) {
  mypic[[i]] <- resize(mypic[[i]], 28 ,28) 
  }

# Reshape
for (i in 1:length(pics)) {
  mypic[[i]] <- array_reshape(mypic[[i]], c(28 ,28,3))
}

#training data 
trainx <- NULL

for (i in 1:length(pics)) {
  trainx <- rbind(trainx, mypic[[i]])
}

str(trainx)
```

```{r}
trainy <- list()
for (i in 1:nrow(imgdata)) {
  if (imgdata$V1[i]=="dog"){
    trainy[i] <-1
  }
  else{
    trainy[i] <-0
  }
}
```


```{r}
trainy1 <- list()
for (i in 1:nrow(train_label)) {
  if (train_label$label[i]=="dog"){
    trainy1[i] <-1
  }
  else{
    trainy1[i] <- 0
  }
}

testy1 <- list()
for (i in 1:nrow(test_label)) {
  if (test_label$label[i]=="dog"){
    testy1[i] <-1
  }
  else{
    testy1[i] <- 0  }
}
#Create data is a 3-d array of grayscale values. To prepare data fro training, it has to convert the 3-d arrays into matrices by reshapinf width dand heights into a single dimension. Then convert the grayscale values from integers ranging between 0 to 255 into floating point value between 0 to 1.

train_total <- merge(train_color,train_HOG,by=c("Image"))
train_total <- merge(train_total,train_LBP,by=c("Image"))


trainx1 <- train_total[1:nrow(train_total),2:ncol(train_total)]
trainx2 <- data.matrix(trainx1, rownames.force = NA)
trainx2 <- unname(trainx2, force = FALSE)


test_total <- merge(test_color,test_HOG,by=c("Image"))
test_total <- merge(test_total,test_LBP,by=c("Image"))


testx1 <- test_total[1:nrow(test_total),2:ncol(test_total)]
testx2 <- data.matrix(testx1, rownames.force = NA)
testx2 <- unname(testx2, force = FALSE)
```

```{r}
trainx1[2:10,2:10]
trainx2[2:10,2:10]
trainx[2:10,2:10]
total[1:10,1:10]


```


```{r}
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
#testlabels <- to_categorical(testy1)
n <- ncol(trainx2)

#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers. 
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352

model <- keras_model_sequential()
model %>%
  layer_dense(units= 256, activation = "relu", input_shape = c(n)) %>%
  layer_dropout(rate = 0.4) %>%
#  layer_dense(units= 512, activation = "relu",use_bias = T) %>%
  layer_dense(units= 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%  
  layer_dense(units= 10, activation = "relu",use_bias = T) %>%


# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid

summary(model)

# Compile 
# The model with approriate loss function, optimizer and metrics
model %>%
  compile(loss ='categorical_crossentropy',
          optimizer = optimizer_rmsprop(),
          metrics = c("accuracy"))

#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually


# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression


# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.

history <- model %>%
  fit(trainx2, 
      trainlabels,
      epochs = 20, 
      batch_size = 32,
      validation_split = 0.2)
#      validation_data = list(testx2, testlabels)) 


# Training visualization
plot(history)
# as.data.frame(history)


```
   


```{r}
#model evalutaion
saveRDS(model, "model.rds")
model %>% evaluate(trainx2,trainlabels)
pred <- model %>% predict_classes(trainx2)
#table(Predict = pred, Actual = trainy1)
prob <-  model %>% predict_proba(trainx2)
#cbind(prob, Predict=pred, Actual=trainy1)
  
```




```{r}
# model testing

model %>% evaluate(testx2,testlabels)
pred <- model%>%predict_classes(testx2)


```



```{r}
load("../output/sift_features_30.RData")
```




```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

train_total <- merge(train_LBP,train_HOG,by=c("Image"))
train_total <- merge(train_total,train_LBP,by=c("Image"))

str(sift.data)
train_total <- train_HOG
train_total <- train_LBP
train_label <- read.table("../output/train_label.txt")

load("../output/sift_features_200.RData")

load("../output/feature_HOG_rotate.RData")
train_total <- dat


trainx <- train_total[1:nrow(train_total),1:ncol(train_total)]
trainx <- data.matrix(trainx, rownames.force = NA)
trainx <- unname(trainx, force = FALSE)


trainy <- list()
for (i in 1:nrow(train_label)) {
  if (train_label$V1[i]=="dog"){
    trainy[i] <-1
  }
  else{
    trainy[i] <- 0
  }
}

testy <- list()
for (i in 1:nrow(test_label)) {
  if (test_label$label[i]=="dog"){
    testy[i] <-1
  }
  else{
    testy[i] <- 0  }
}

```



```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255




```


```{r}


#y_train <- to_categorical(y_train, 10)
#y_test <- to_categorical(y_test, 10)


#trainy <- to_categorical(trainy, 2)
trainx1 <- rbind(trainx[1:1000,],trainx[1401:2000,])
trainy1 <- rbind(trainy[1:1000,],trainy[1401:2000,])
testx1 <- trainx[1001:1400,]
testy1 <- trainy[1001:1400,]

k = 10
cross = data.frame(loss=1:10,acc=1:10,testloss=1:10,testacc=1:10)


pptrainy<-trainy
trainy<-pptrainy

trainy <- to_categorical(trainy, 2)
trainy <- trainy[,2]
trainy <- as.data.frame(trainy)
trainy <- rbind(trainy,trainy)
trainy1 <- trainy
train_total1 <- cbind(trainy1,train_total)



for (i in 1:10) {
  
  
train.index <- createDataPartition(train_total1$trainy, p = .8, list = FALSE)
trainx1 <- train_total1[ train.index,2:1765]
trainy1 <- train_total1[train.index,1]
testx1  <- train_total1[-train.index,2:1765]
testy1  <- train_total1[-train.index,1]

trainx1 <- data.matrix(trainx1, rownames.force = NA)
trainx1 <- unname(trainx1, force = FALSE)
testx1 <- data.matrix(testx1, rownames.force = NA)
testx1 <- unname(testx1, force = FALSE)

trainy1 <- to_categorical(trainy1, 2)
testy1 <- to_categorical(testy1, 2)



model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(1764)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'softmax')


model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)


  history <- model %>% fit(
   trainx1, trainy1, 
   epochs = 50, batch_size = 50 
  # validation_split = 0.2
  )
  acc <- model %>% evaluate(trainx1,trainy1)
  
  testacc <- model %>% evaluate(testx1, testy1)
  
  cross$loss[i] <- acc$loss
  cross$acc[i] <- acc$acc
  cross$testloss[i] <- testacc$loss
  cross$testacc[i] <- testacc$acc
 rm(model)
}

pred <- model %>% predict_classes(trainx)


```


```{r}
  

k = 200
cross = data.frame(loss=1:200,acc=1:200,testloss=1:200,testacc=1:200)

for (i in 20:200) {
  
history <- model %>% fit(
   trainx1, trainy1, 
   epochs = 60, batch_size = 50 
  # validation_split = 0.2
  )
  acc <- model %>% evaluate(trainx,trainy)
  
  testacc <- model %>% evaluate(testx1, testy1)
  
  cross$loss[i] <- acc$loss
  cross$acc[i] <- acc$acc
  cross$testloss[i] <- testacc$loss
  cross$testacc[i] <- testacc$acc
}

t<-cross[20:90,]
p1 <- ggplot(t, aes(x = 20:90, y = testacc))+
  geom_line()
```





