---
title: "Random Forest"
author: "Daniel Joseph Parker"
date: "2/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Import the training and test data
```{r}
train_color <- load("../data/split_data/train/train_color.Rdata")
train_HOG <- load("../data/split_data/train/train_HOG.Rdata")
train_label <- load("../data/split_data/train/train_label.Rdata")
train_LBP <- load("../data/split_data/train/train_LBP.Rdata")

test_color <- load("../data/split_data/test/test_color.Rdata")
test_HOG <- load("../data/split_data/test/test_HOG.Rdata")
test_label <- load("../data/split_data/test/test_label.Rdata")
test_LBP <- load("../data/split_data/test/test_LBP.Rdata")
```
2. Create base "train_rf" function.

3. Run cross validation over parameters = number of trees, anything else? to select best parameters.
a. Create loop to create tree using train_rf for each set.
b. Determine best parameters by comparing performance metric.

4. Train final tree with best parameters using train_rf.

5. Record the final accuracy and the amount of time needed to construct tree and make predictions.

```{r Train Random Forest, eval=FALSE, include=FALSE}

library(randomForest)

train_rf = function(dat_train){
  
  ###  dat_train: processed features from images also contains label
  
  fitControl = trainControl(method = 'cv', number = 2)
  
  rfGrid = expand.grid(mtry = floor(sqrt(ncol(dat_train)) * 0.95) : floor(sqrt(ncol(dat_train) * 1.05)))
  
  start_time_rf = Sys.time() # Model Start Time 
  rf.fit = train(Label~., 
                 data = dat_train,
                 method = "rf", 
                 trControl = fitControl,
                 ntree = 500, #number of trees to grow
                 tuneGrid = rfGrid) # Parameter Tuning
  end_time_rf = Sys.time() # Model End time
  end_time_rf - start_time_rf
  
  rf_time = end_time_rf - start_time_rf #Total Running Time
  
  return(list(fit = rf.fit, time = rf_time))
}
```

```{r Random Forest Version 2, eval=FALSE, include=FALSE}


raw_data <- read.csv('../data/feature.csv',header = F,skip = 1)
raw_data <- raw_data[,2:ncol(raw_data)]
raw_data$y <- as.factor(read.csv('../data/label_train.csv',header = F,skip = 1)[,2])

# build model
train.rf<- function(traindata) {
  
  traindata$y<- as.factor(traindata$y)
  y.index<- which(colnames(traindata)=="y")
  bestmtry <- tuneRF(y= traindata$y, x= traindata[,-y.index], stepFactor=1.5, improve=1e-5, ntree=600)
  best.mtry <- bestmtry[,1][which.min(bestmtry[,2])]
  
  model.rf <- randomForest(y ~ ., data = traindata, ntree=600, mtry=best.mtry, importance=T)
  return(model.rf)
}
# predict model
test.rf <- function(model,test.data) {
  return(predict(model, test.data, type = "class"))
}

# main_part : k fold cross validation
K <- 5
n <- nrow(raw_data)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))

cv.error.rf <- rep(NA, K)

for (i in 1:K){
  print(i)

  train.data <- raw_data[s != i,]
  test.data <- raw_data[s == i,]
  
  fit.rf <- train.rf(train.data)
  pred.rf <- test.rf(fit.rf, test.data)  
  cv.error.rf[i] <- mean(pred.rf != test.data$y)

}			
cv.error.rf


# test part
#rf <- train.rf(traindata = raw_data[1:1000,])
#fit.rf <- train.rf(train_data)
#pred.rf <- test.rf(fit.rf, test_data)  
#accuracy<- mean(pred.rf != test_data$y)
```
