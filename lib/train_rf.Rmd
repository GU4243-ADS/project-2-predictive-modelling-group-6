---
title: "Random Forest Model Training"
author: "Daniel Joseph Parker"
date: "2/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import training and test data

+ Inside the `/data/` subdirectory in the project structure, there are separate folders for training and test data according to a randomly generated, reproducible 80% - 20% split, as discussed in our group's 2-26-18 meeting.

+ Within each folder, there are several files:
++ labels
++ 3 `.Rdata` files: color features, HOG features, and LBP features.

+ Still to incorporate: SIFT features. Currently, these have not yet been split with the others.

```{r Load data}
# Training labels:
# train_label
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/train/train_label.Rdata")

# Training data:
# train_color
# train_HOG
# train_LBP
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/train/train_color.Rdata")
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/train/train_HOG.Rdata")
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/train/train_LBP.Rdata")

# Test labels:
# test_label
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/test/test_label.Rdata")

# Test data:
# test_color
# test_HOG
# test_LBP
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/test/test_color.Rdata")
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/test/test_HOG.Rdata")
load("/Users/djparker/Documents/GitHub/project-2-predictive-modelling-group-6/data/split_data/test/test_LBP.Rdata")
```

# Data preprocessing

To tune and train the random forest model, we will need to preprocess the training and test data such that the adhere to the following form:

+ Row names: consistent row numbers each corresponding to the same image
+ 1st column: the labels! ("cat", "dog")
+ 2nd column: retain original image name (in case refering to the original image becomes important)
+ The remainder of the dataframe is the numerical features. These are the predictors we'll build the model on.

## Row names

The row names pre-randomized split are still intact, so first we replace the row names. (Upon inspection, we can see that the labels have the same *ordering* as the feature datasets; thus, this step is justified.)

```{r Row name concordance between labels and features}
rownames(train_color) <- 1:1600
rownames(train_HOG) <- 1:1600
rownames(train_LBP) <- 1:1600

rownames(test_color) <- 1:400
rownames(test_HOG) <- 1:400
rownames(test_LBP) <- 1:400
```

## Image names

Each feature dataframe has the original image names as its first column. We only need to retain one. So we'll delete this column from 2 of the 3 feature dataframes. (Arbitrarily, we'll keep that column in the `LBP` dataframes, and put that first in line for merging.)

```{r Retain only one image name column}
require(tidyverse, quietly = TRUE) # For the pipeline operator %>%

train_HOG <-
  train_HOG %>%
  select(-Image)

train_color <-
  train_color %>%
  select(-Image)

test_HOG <-
  test_HOG %>%
  select(-Image)

test_color <-
  test_color %>%
  select(-Image)
```

## Merge!

This step produces `train_data`, the final input into the model building function. It has the form described in the top of this section.

```{r}
train_data <- cbind(train_label, train_LBP, train_HOG, train_color)
test_data <- cbind(test_label, test_LBP, test_HOG, test_color)
```

Our data are now ready to generate random forests with!

# Train the optimum random forest model

## Parameter tuning

To write: description of how the `caret` package is used here to find optimal model.
```{r}
require(caret, quietly = TRUE)
require(randomForest, quietly = TRUE)

# This set-up draws from https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/
control <- trainControl(method = "repeatedcv",
                        number = 10,
                        repeats = 3,
                        search = "random")

seed = 1
set.seed(seed)

# This is the computationally intensive step.
start_time = Sys.time()
rf <- train(label ~ .,
            data = train_data[-Image],
            method = "rf",
            tuneLength = 15,
            ntree = 500,
            trControl = control)
end_time = Sys.time()
build_time = end_time - start_time
}

# When run, this code should show the accuracy.
```

## Save the model

```{r Save model}
# To write:
# Save the object to /output/ folder
```

# Alternate

```{r Random forest generation, eval=FALSE, include=TRUE}
# This is an alternate version of optimizing the random forest
# that uses the built-in `tuneRF` function in the 
# `randomForest` package to optimize the `mtry` paramter,
# which is the most important parameter to search over in random forests.

require(randomForest, quietly = TRUE)

seed = 1
set.seed(seed)

response <- training_data$label
predictors <- training_data[3:length(training_data)]

rf_alternate <- tuneRF(predictors, response, ntreeTry = 500, doBest = TRUE))
```