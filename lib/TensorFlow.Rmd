---
title: "Classification method"
author: "Kai"
date: "February 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(reticulate)

# If you are using anaconda, point reticulate to the correct conda environment
# use_condaenv('your-environment')

# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
#cv2 <- reticulate::import('cv2')

library(EBImage)
library(tensorflow)
library(keras)

```

```{r}
# Read the seperated data
load("../data/split_data/train/train_HOG.RData")
load("../data/split_data/train/train_label.RData")
load("../data/split_data/train/train_color.RData")
load("../data/split_data/train/train_LBP.RData")

load("../data/split_data/test/test_HOG.RData")
load("../data/split_data/test/test_label.RData")
load("../data/split_data/test/test_color.RData")
load("../data/split_data/test/test_LBP.RData")
```
##Read images


```{r}
# set the working direction
#setwd("../data/pets_training/pets/train")
# load pictures one by one
#pics <- c("sss")


ID <- list(1:2000)
label <- read.table("../data/pets_training/pets/train_label.txt")

imgdata <- data.frame(ID,label)
s1 <- "pet"
s3 <- ".jpg"

pics <- list()
for(i in 1:2000){
  s2 <- toString(i)
  pics[[i]] <- paste(s1, s2, s3,sep = "")
}


mypic <- list()
for (i in 1:length(pics)) {
  mypic[[i]] <- readImage(paste("../data/pets_training/pets/train/",pics[i],sep=""))
  }


```

# Data Processing
```{r}
# Resize
for (i in 1:length(pics)) {
  mypic[[i]] <- resize(mypic[[i]], 28 ,28) 
  }

# Reshape
for (i in 1:length(pics)) {
  mypic[[i]] <- array_reshape(mypic[[i]], c(28 ,28,3))
}

#training data 
trainx <- NULL

for (i in 1:length(pics)) {
  trainx <- rbind(trainx, mypic[[i]])
}

str(trainx)
```

```{r}
trainy <- list()
for (i in 1:nrow(imgdata)) {
  if (imgdata$V1[i]=="dog"){
    trainy[i] <-1
  }
  else{
    trainy[i] <-0
  }
}
```


```{r}
trainy1 <- list()
for (i in 1:nrow(train_label)) {
  if (train_label$label[i]=="dog"){
    trainy1[i] <-1
  }
  else{
    trainy1[i] <-0
  }
}

testy1 <- list()
for (i in 1:nrow(test_label)) {
  if (test_label$label[i]=="dog"){
    testy1[i] <-1
  }
  else{
    testy1[i] <-0
  }
}
#Create data is a 3-d array of grayscale values. To prepare data fro training, it has to convert the 3-d arrays into matrices by reshapinf width dand heights into a single dimension. Then convert the grayscale values from integers ranging between 0 to 255 into floating point value between 0 to 1.

train_total <- merge(train_color,train_HOG,by=c("Image"))
train_total <- merge(train_total,train_LBP,by=c("Image"))


trainx1 <- train_total[1:nrow(train_total),2:ncol(train_total)]
trainx2 <- data.matrix(trainx1, rownames.force = NA)
trainx2 <- unname(trainx2, force = FALSE)


test_total <- merge(test_color,test_HOG,by=c("Image"))
test_total <- merge(test_total,test_LBP,by=c("Image"))


testx1 <- test_total[1:nrow(test_total),2:ncol(test_total)]
testx2 <- data.matrix(testx1, rownames.force = NA)
testx2 <- unname(testx2, force = FALSE)
```

```{r}
trainx1[2:10,2:10]
trainx2[2:10,2:10]
trainx[2:10,2:10]
total[1:10,1:10]


```


```{r}
#One Hot Encoding
trainlabels <- to_categorical(trainy1)
testlabels <- to_categorical(testy1)
n <- ncol(trainx1)

#Create the model (where to recreate the model)
# the simplest type of model is the sequential model, a linear stack of layers. 
# Imput_shape is the first layer specifies the shapre of the input data. 28*28*3=2352

model <- keras_model_sequential()
model %>%
  layer_dense(units= 256, activation = "relu", input_shape = c(n)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units= 256, activation = "relu") %>%
  layer_dense(units= 128, activation = "relu") %>%
#  layer_dropout(rate = 0.3) %>%
  layer_dense(units= 2, activation = "sigmoid")

# The final layer outputs is binary by using softmax activation function.
# can chang ethe activation fucntion such as sigmoid

summary(model)

# Compile 
# The model with approriate loss function, optimizer and metrics
model %>%
  compile(loss ='binary_crossentropy',
          optimizer = "sgd",
          metrics = c("accuracy"))

#optimizer_rmsprop(),
#SGD()
#          metrics = metric_binary_accuracy)
# This one used to binary classfication usually


# model %>%
#  compile(loss ='mse',
#          optimizer = 'optimizer_rmsprop(lr = 0.002)',
#          metrics = c("accuracy"))
# This one like a mean squared error regression


# Fit Model
# Train the model for 30 epochs -- one forward poass and one backward pass of all the training examples
# Batch size -- the nuber of training examples in one forward/backward pass. the higher batch size, the more memory space required.

history <- model %>%
  fit(trainx2, 
      trainlabels,
      epochs = 30, 
      batch_size = 32,
      validation_split = 0.2) #20% data used for training


# Training visualization
plot(history)
# as.data.frame(history)


```
   


```{r}
#model evalutaion

model %>% evaluate(trainx2,trainlabels)
pred <- model %>% predict_classes(trainx2)
#table(Predict = pred, Actual = trainy1)
prob <-  model %>% predict_proba(trainx2)
cbind(prob, Predict=pred, Actual=trainy1)
  
```

```{r}
# model testing

model %>% evaluate(testx2,testlabels)
pred <- model%>%predict_classes(testx2)


```








