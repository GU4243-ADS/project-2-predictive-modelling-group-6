\documentclass[serif]{beamer}
\usetheme{Frankfurt}
\usecolortheme{crane}
\usefonttheme{structuresmallcapsserif}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows,chains}

\tikzset{
  startstop/.style={
    rectangle, 
    rounded corners,
    minimum width=3cm, 
    minimum height=1cm,
    align=center, 
    draw=black, 
    fill=red!30
    },
  arrow/.style={thick,->,>=stealth}
}

\title{Cat or Dog: Predictive Modeling}
\subtitle{STAT GU4243 - Applied Data Science}
\author{Group 6}
\institute{Columbia University}
\date{\today}

\begin{document}

\frame{\titlepage}

\section{Outline}
\frame{\tableofcontents}

\section{Introduction}
\subsection{Us}
\frame{
	\frametitle{Group members}
	Wanting Cheng, Mingkai Deng, Jiongjiong Li, Kai Li, Daniel Parker
}
\subsection{Motivation}
\frame{
	\frametitle{Why do this?---Motivation}
	\includegraphics[width=\linewidth, height=\textheight,keepaspectratio]{../../../../../Downloads/pet47.jpg}
}
\frame{
	\frametitle{Why do this?---Motivation}
	\includegraphics[width=\linewidth, height=\textheight,keepaspectratio]{../../../../../Downloads/pet197.jpg}
}

\subsection{Scope}
\frame{
	\frametitle{Spec \& Scope}
	[C]\textit{arry out model evaluation and selection for predictive analytics on image data}
	$\ldots$
	[using] \textit{a set of 4387 labeled images of cats and dogs}
	$\ldots$
	\textit{creat}[e] \textit{a mobile AI program that accurately distinguishes between} [them]
	$\ldots$
	\textit{balance between the complexity of variables/features/models used and the predictive performance.}
}
\frame{
	\frametitle{Spec \& Scope}	
	\includegraphics[width=\linewidth, height=\textheight,keepaspectratio]{../../../../../Downloads/predictiveprogram.png}
}

\section{Method}
\subsection{Exploratory analysis}
\frame{
	\frametitle{Exploratory Analysis}
	What makes one animal different from another?
	[Intuition]
	
	What approaches did previous semesters' groups employ?
	[Research]
}

\subsection{Feature extraction}
\frame{
	\frametitle{Feature Extraction}
	\begin{enumerate}
	\item<1-> SIFT = scale-invariant feature transformation.
	\item<2-> HOG = histogram of oriented gradients.
	\item<3-> LBP = local binary patterns.
	\item<4-> HSV = hue, saturation, value.
	\item<5-> RGB = red, green, blue.
	\end{enumerate}
}
\subsection{Statistical machine learning models}
\frame{
	\frametitle{Statistical Machine Learning Models}
	\begin{enumerate}
	\item<1-> Gradient boosting machine---the baseline.
	\item<2-> Random forests.
	\item<3-> TensorFlow/Keras neural network.
	\item<4-> Support vector machine.
	\item<5-> Adaptive boosting (``AdaBoost").
	\item<6-> Extreme gradient boosting (``XGBoost").

	\end{enumerate}}

\subsection{Tuning and training}
\frame{
	\frametitle{Tuning and Training}
	Simplifying heuristic: use \textit{all} features, rather than subsets.
		
	Preference for built-in package functions, rather than a generalized syntax.
}
\frame{
	\frametitle{How we finalized models---Flowchart}
	\centering
\begin{tikzpicture}[
  start chain=going below,
  every join/.style={arrow},
  node distance=0.6cm
  ]
\node (step1) [startstop,on chain,join] {Training dataset};
\node (step2) [startstop,on chain,join] {Random reproducible test/train validation split};
\node (step3) [startstop,on chain,join] {Tune models using train};
\node (step4) [startstop,on chain,join] {Estimate model accuracy using test};
\node (step5) [startstop,on chain,join] {Train final models, with optimum parameters, using \textit{all} data};
\end{tikzpicture}	
}
\section{Results}
\frame{
	\frametitle{Results}
	\includegraphics[width=\linewidth, height=\textheight,keepaspectratio]{../../../../../Downloads/predictiveprogram.png}
}

\subsection{Feature extraction}
\frame{
	\frametitle{Feature Extraction Time}
	\begin{table}
	\begin{tabular}{r | c | c | c }
	Feature type & Color & HOG & LBP \\ 
	\hline
	Time (m) & 7 & 5 & 20
	\end{tabular}
	\caption{Image processing time by feature, in minutes}
	\end{table}
}

\subsection{Computational cost}
\frame{
	\frametitle{Training Times---Computational Cost}
	\begin{table}
	\begin{tabular}{r | c | c | c | c}
	
	Model & SIFT & Color & HOG & LBP \\
	\hline
	GBM & 13.452 & 89.036 & 116.964 & 2.74\\ 
	RF & 174.1 & 905.883 & 1999.23 & 24.842\\
	NN & 31 & 65.81 & 61.58 & 28.12 \\
	SVM  & 3.484 & & 33.099 & 0.805\\  
	XGBoost & 3.829 & 16.851 & 27.005 & 1.67 \\ 
	AdaBoost & 16.34 & 103.61 & 142.45 & 3.25
	
	\end{tabular}
	\caption{Training time per model, in seconds}
	\end{table}
}

\subsection{Prediction accuracy}
\frame{
	\frametitle{Prediction Accuracy}
	\begin{table}
	\begin{tabular}{r | c | c | c | c}
	
	Model & SIFT & Color & HOG & LBP \\
	\hline
	GBM & 73.25 & 69.5 & 75.25 & 69.\\
	RF & 72.25 & 73. & 74.25 & 69.\\
	NN & 75.75 & 64.5 & 76.75 & 69.\\
	SVM & 77.5 & & 77.5 & 69.75\\
	XGBoost  & 72 & 72 & 77.25 & 66.5\\
	AdaBoost & 72.75 & 69.5 & 71.75 & 69.75
	
	\end{tabular}
	\caption{Prediction accuracy by model, in percentage}
	\end{table}
}

\subsection{Final result}
\frame{
	\frametitle{Final Result}
	Our final feature extraction and model choice is HOG features with XGBoost. This choice optimizes extraction time, training time, and prediction accuracy.
}

%\frame{
%	\frametitle{Comparison}
%% ggplot visualization scatterplot: 
%%% x axis: time to make prediction
%%% y axis: accuracy in percentage
%}

\section{Discussion}
\subsection{Confusion matrices}
\frame{
	\frametitle{Confusion Matrices: Random Forest}
	\begin{table}
	\begin{tabular}{r | c | c }
	RF Prediction & Dog & Cat\\
	\hline
	Truth & &\\
	Dog & 54 & 19\\
	Cat & 92 & 35
	\end{tabular}
	\caption{RF-SIFT Confusion Matrix}
	\end{table}
		\begin{table}
	\begin{tabular}{r | c | c }
	RF Prediction & Dog & Cat\\
	\hline
	Truth & &\\
	Dog & 247 & 26\\
	Cat & 77 & 50
	\end{tabular}
	\caption{RF-HOG Confusion Matrix}
	\end{table}
}

\frame{
	\frametitle{Confusion Matrices: Gradient Boosting Machine}
	\begin{table}
	\begin{tabular}{r | c | c }
	GBM Prediction & Dog & Cat\\
	\hline
	Truth & &\\
	Dog & 229 & 44\\
	Cat & 63 & 64
	\end{tabular}
	\caption{GBM-SIFT Confusion Matrix}
	\end{table}
	\begin{table}
	\begin{tabular}{r | c | c }
	GBM Prediction & Dog & Cat\\
	\hline
	Truth & &\\
	Dog & 232 & 41\\
	Cat & 58 & 69
	\end{tabular}
	\caption{GBM-HOG Confusion Matrix}
	\end{table}
}

\frame{
	\frametitle{Confusion Matrices: Neural Network}
	\begin{table}
	\begin{tabular}{r | c | c }
	NN Prediction & Dog & Cat\\
	\hline
	Truth & &\\
	Dog & 233 & 40\\
	Cat & 55 & 72
	\end{tabular}
	\caption{NN-SIFT Confusion Matrix}
	\end{table}
	\begin{table}
	\begin{tabular}{r | c | c }
	NN Prediction & Dog & Cat\\
	\hline
	Truth & &\\
	Dog & 263 & 10\\
	Cat & 89 & 38
	\end{tabular}
	\caption{NN-HOG Confusion Matrix}
	\end{table}
}

\subsection{Further directions}
\frame{
	\frametitle{Further directions to explore}
	\begin{enumerate}
	\item<1-> Other extractions and combinations thereof.
	\item<2-> Dataset manipulation to ``grow" more training data for free.
	\item<3-> Other models.
	\item<4-> Ensembling.
	\item<5-> $\ldots$
	\end{enumerate}
}

\frame{
Thank you!
}

\end{document}